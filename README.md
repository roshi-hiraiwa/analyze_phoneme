# 音声とヒートマップによる音素解析ツール（未完成）
機械学習モデルが出力したヒートマップ（画像の赤色領域）と、音声データのタイムスタンプを照合し、注目領域の文字起こし判定を行うための解析プログラムです。<br>
※Google Text-to-Speech API v1beta1において、特定の単語が含まれる際に音声生成が失敗する不具合（API側のバグ）を確認したため、技術検証フェーズで開発を終了しました。(2025年12月時点)

## 実装したロジック
### 1. 音声データ生成（MATLAB）
1. テキスト→音声変換(`Mylib.m`)<br>
   - Google Text-to-Speech API v1beta1を使用
   - 日本語テキストを音節単位に分解（拗音・長音対応）し、SSML形式で各音節に`<mark>`タグを付与
   - 音声サンプリングレート：24000Hz固定（50msの無音マージンを先頭に追加）

2. MFCC特徴量抽出(`make_mfcc_csv.m`)<br>
   - MIRtoolboxを使用してMFCC係数を計算
   - フレーム幅：0.1秒、係数ランク：1-6
   - 出力次元：1×240 (40フレーム×6係数)
   - タイムスタンプCSVをShift_JIS/UTF-8で保存

### 2. 学習モデル構築（Python）
- CNN学習・評価(`make_cnn_classification_loo.py`)<br>
   - Leave-One-Out交差検証（LOO）で学習
   - エポック数：200、バッチサイズ：30
   - Grad-CAMを用いて判断根拠となるヒートマップを生成

### 3. データ処理（Python）
1. TP/TN抽出(`analyze_phoneme.py`)<br>
   - `train_data.csv`（正解ラベル）と`target/result.csv`（予測確率）を結合
   - True Positive(TP)とTrue Negative(TN)のサンプルを抽出

2. ヒートマップ画像処理<br>
   - Grad-CAMヒートマップから指定座標(80-478, 213-273)で切り取り
   - 切り取った画像を`target/heatmap/`に保存

3. 赤色領域検出<br>
   - HSV色空間で赤色領域を検出(H：0-5°, S：100-255, V：100-255)
   - 輪郭抽出によりバウンディングボックスを取得
   - ピクセル位置を時間（秒）に変換（画像幅398px = 2.083秒）

4. タイムスタンプ照合<br>
   - 赤色領域の時間範囲と、APIから取得した各音素の時間範囲を比較
   - 重なりがある音素を抽出して文字列として出力

5. 可視化<br>
   - ヒートマップ画像にタイムスタンプ区間（青）と赤色領域（緑）を重ねて表示
   - 日本語フォント（MS Gothic）で音素文字を描画
   - 比較画像を`target/validation/`に保存
